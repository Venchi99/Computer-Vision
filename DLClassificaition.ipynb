{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a620aeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1df4b1434d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc8b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6896bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path\n",
    "train_imgs_file = \"data/kmnist-npz/kmnist-train-imgs.npz\"\n",
    "train_labels_file = \"data/kmnist-npz/kmnist-train-labels.npz\"\n",
    "\n",
    "val_imgs_file = \"data/kmnist-npz/kmnist-val-imgs.npz\"\n",
    "val_labels_file = \"data/kmnist-npz/kmnist-val-labels.npz\"\n",
    "\n",
    "test_imgs_file = \"data/kmnist-npz/kmnist-test-imgs.npz\"\n",
    "test_labels_file = \"data/kmnist-npz/kmnist-test-labels.npz\"\n",
    "\n",
    "# Parameters\n",
    "batchsize = 64 #64\n",
    "epochs = 14\n",
    "lr = 0.001  #smaller -> more likely to overfit\n",
    "betas = (0.9, 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f3177ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ( 'お','き','す','つ','な','は','ま','や','れ','を')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35e3a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transfrom for training set, composes several transforms together\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    #randomly flip the image left and right\n",
    "    transforms.RandomHorizontalFlip(p=0.5), \n",
    "    transforms.ToTensor(),\n",
    "    # normalize the data to the range between (-1, 1)\n",
    "    transforms.Normalize(mean=0.5, std=0.5),\n",
    "    # zero-pad 4 pixels\n",
    "    transforms.Pad([4, 4]),\n",
    "    #randomly crop image to 28x28\n",
    "    transforms.RandomCrop(28),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f169a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define transfrom for validation set & test set\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # normalize the data to the range between (-1, 1)\n",
    "    transforms.Normalize(mean=0.5, std=0.5),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6115bcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, imgs_file, labels_file, transform=None):\n",
    "        self.imgs = np.load(imgs_file)['arr_0']\n",
    "        self.img_labels = np.load(labels_file)['arr_0']\n",
    "        self.img_dir = imgs_file\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.imgs[idx]\n",
    "        label = self.img_labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5150d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of dataset and load training, validation, test sets\n",
    "training_set = MyDataset(train_imgs_file, train_labels_file, transform)\n",
    "train_loader = DataLoader(training_set, batch_size = batchsize, shuffle = True)\n",
    "\n",
    "val_set = MyDataset(val_imgs_file, val_labels_file, transform2)\n",
    "val_loader = DataLoader(val_set, batch_size = batchsize, shuffle = True)\n",
    "\n",
    "test_set = MyDataset(test_imgs_file, test_labels_file, transform2)\n",
    "test_loader = DataLoader(test_set, batch_size = batchsize, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3303fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 64\n",
    "f2 = 128\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 5×5 Convolutional Layer with 32 filters, stride 1 and padding 2.\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, stride = 1,padding =  2) #input channels, output channels: # of filters, kernel_size\n",
    "        # 2×2 Max Pooling Layer with a stride of 2.\n",
    "        self.max_pooling = nn.MaxPool2d(2, stride = 2)\n",
    "        #3×3 Convolutional Layer with 64 filters, stride 1 and padding 1.\n",
    "        self.conv2 = nn.Conv2d(f1, f2, 3, stride = 1,padding =  1)\n",
    "        #Fully-connected layer with 1024 output units.\n",
    "        self.fc1 = nn.Linear(f2*7*7, 1024) #output channels*image size\n",
    "        # Second fully connected layer that outputs our 10 labels\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "        self.dropout = nn.Dropout(p = 0.2)\n",
    "        self.batchnorm = nn.BatchNorm2d(f2)\n",
    "        self.batchnorm32 = nn.BatchNorm2d(f1)\n",
    "        self.conv3 = nn.Conv2d(f2, f2, 3, stride = 1,padding =  1)\n",
    "        self.conv4 = nn.Conv2d(32, f1, 3, stride = 1,padding =  1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batchnorm32(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pooling(x)\n",
    "        x= self.conv2(x)\n",
    "       #  x = self.batchnorm(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pooling(x)\n",
    "        x = torch.flatten(x, 1) #flatten all except batch\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x= F.relu(self.fc1(x))\n",
    "       # x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x#F.log_softmax(x, dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfbd5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 5×5 Convolutional Layer with 32 filters, stride 1 and padding 2.\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, stride = 1,padding =  2) #input channels, output channels: # of filters, kernel_size\n",
    "        # 2×2 Max Pooling Layer with a stride of 2.\n",
    "        self.max_pooling = nn.MaxPool2d(2, stride = 2)\n",
    "        #3×3 Convolutional Layer with 64 filters, stride 1 and padding 1.\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride = 1,padding =  1)\n",
    "        #Fully-connected layer with 1024 output units.\n",
    "        self.fc1 = nn.Linear(64*7*7, 1024) #output channels*image size\n",
    "        # Second fully connected layer that outputs our 10 labels\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.max_pooling(x)\n",
    "        x= F.relu(self.conv2(x))\n",
    "        x = self.max_pooling(x)\n",
    "        x = torch.flatten(x, 1) #flatten all except batch\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x= F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "612d8314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a instance of Model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "22dcf01f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "output = model(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f91e3bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bbbceba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,predicted1 = torch.max(output.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "69105f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([-4.0486, -4.0971, -4.0344, -4.0348, -4.0655, -4.0902, -4.0567, -4.0746,\n",
       "        -4.1026, -4.0965, -4.1252, -4.1157, -4.0640, -4.0466, -4.0253, -4.0702,\n",
       "        -4.1113, -4.0086, -4.1035, -4.0846, -4.1179, -4.0786, -4.0855, -4.0877,\n",
       "        -4.0754, -4.0897, -4.0770, -4.1156, -4.0987, -4.0990, -4.0846, -4.0591,\n",
       "        -4.0843, -4.0714, -4.1043, -4.0841, -4.0740, -4.0472, -4.1000, -4.1300,\n",
       "        -4.0722, -4.0738, -4.0878, -4.0576, -4.0558, -4.0232, -4.1070, -4.1022,\n",
       "        -4.0636, -4.0758, -4.0520, -4.0833, -4.0913, -4.0698, -4.0569, -4.0739,\n",
       "        -4.1121, -4.0733, -4.0521, -4.0992, -4.0693, -4.1051, -4.0607, -4.0767]),\n",
       "indices=tensor([2, 9, 5, 2, 1, 1, 3, 6, 3, 3, 2, 6, 8, 8, 6, 5, 2, 8, 8, 7, 8, 4, 0, 8,\n",
       "        8, 6, 3, 1, 0, 1, 1, 6, 0, 7, 2, 6, 0, 7, 4, 2, 0, 5, 8, 5, 4, 8, 4, 8,\n",
       "        8, 0, 1, 9, 8, 1, 6, 9, 3, 3, 0, 7, 9, 1, 5, 6]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(output.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3effe271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 5, 5, 0, 1, 8, 1, 4, 4, 1, 4, 0, 4, 2, 1, 8, 0, 4, 8, 1, 8, 3, 5,\n",
       "        5, 5, 3, 2, 7, 4, 5, 9, 3, 6, 8, 3, 5, 2, 9, 2, 4, 1, 5, 7, 4, 2, 8, 2,\n",
       "        1, 9, 6, 0, 0, 6, 9, 3, 3, 7, 7, 3, 6, 1, 2, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01c7629a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 3, 1, 3, 2, 0, 2, 0, 0, 8, 6, 4, 3, 6, 0, 6, 2, 0, 6, 0, 3, 1, 9, 1,\n",
       "        8, 6, 6, 7, 3, 7, 4, 3, 5, 2, 8, 1, 3, 0, 8, 7, 2, 7, 2, 3, 4, 5, 3, 5,\n",
       "        9, 4, 6, 5, 1, 3, 6, 1, 8, 1, 5, 7, 4, 7, 3, 6], dtype=torch.uint8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f93fa642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d5900fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.view(output.size(0), -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b9344b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0152, 0.0147, 0.0152, 0.0158, 0.0164, 0.0157, 0.0167, 0.0161, 0.0166,\n",
       "        0.0150], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2b14ba83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_acc(output, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4c91f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cross-entropy loss\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a118aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a instance of Model\n",
    "model = Model()\n",
    "\n",
    "# Set up Adam optimizer, with 1e-3 learning rate and betas=(0.9, 0.999).\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, betas = (0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "090a05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(outputs, labels): \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    _,predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    acc = torch.div(correct*100, total, rounding_mode='trunc')\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be68df8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting validation for epoch 1\n",
      "validation loss for epoch  1: 0.259056\n",
      "validation acc for epoch  1: 91.000000\n",
      "starting validation for epoch 2\n",
      "validation loss for epoch  2: 0.179389\n",
      "validation acc for epoch  2: 94.000000\n",
      "starting validation for epoch 3\n",
      "validation loss for epoch  3: 0.149584\n",
      "validation acc for epoch  3: 95.000000\n",
      "starting validation for epoch 4\n",
      "validation loss for epoch  4: 0.139131\n",
      "validation acc for epoch  4: 95.000000\n",
      "starting validation for epoch 5\n",
      "validation loss for epoch  5: 0.119837\n",
      "validation acc for epoch  5: 96.000000\n",
      "starting validation for epoch 6\n",
      "validation loss for epoch  6: 0.100235\n",
      "validation acc for epoch  6: 97.000000\n",
      "starting validation for epoch 7\n",
      "validation loss for epoch  7: 0.118195\n",
      "validation acc for epoch  7: 97.000000\n",
      "starting validation for epoch 8\n",
      "validation loss for epoch  8: 0.100965\n",
      "validation acc for epoch  8: 97.000000\n",
      "starting validation for epoch 9\n",
      "validation loss for epoch  9: 0.083324\n",
      "validation acc for epoch  9: 97.000000\n",
      "starting validation for epoch 10\n",
      "validation loss for epoch 10: 0.112881\n",
      "validation acc for epoch 10: 97.000000\n",
      "starting validation for epoch 11\n",
      "validation loss for epoch 11: 0.079160\n",
      "validation acc for epoch 11: 97.000000\n",
      "starting validation for epoch 12\n",
      "validation loss for epoch 12: 0.084597\n",
      "validation acc for epoch 12: 98.000000\n",
      "starting validation for epoch 13\n",
      "validation loss for epoch 13: 0.076946\n",
      "validation acc for epoch 13: 97.000000\n",
      "starting validation for epoch 14\n",
      "validation loss for epoch 14: 0.080546\n",
      "validation acc for epoch 14: 97.000000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "best_ind = -1\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        #writer.add_scalar(\"Train Loss vs Epoches\", loss, epoch)\n",
    "        #writer.add_scalar(\"Train Accuracy(%) vs Epoches\", cal_acc(outputs, labels), epoch)\n",
    "\n",
    "    \n",
    "        #print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _,predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        \n",
    "        \n",
    "    train_loss = running_loss/len(train_loader)\n",
    "    writer.add_scalar(\"Train Loss vs Epochs\", train_loss, epoch)    \n",
    "    \n",
    "    acc = torch.div(correct*100, total, rounding_mode='trunc')\n",
    "    writer.add_scalar(\"Train Accuracy(%) vs Epochs\", acc, epoch)\n",
    "    \n",
    "    PATH = './checkpoints/cifar_net{:02d}.pth'.format(epoch)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    #validation\n",
    "    validation_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(\"starting validation for epoch {}\".format(epoch+1))\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            #TODO\n",
    "            loss = loss_func(outputs, labels)\n",
    "           \n",
    "            validation_loss += loss.item()\n",
    "            _,predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "\n",
    "        validation_loss /= len(val_loader)\n",
    "        writer.add_scalar(\"Validation Loss vs Epochs\", validation_loss, epoch)\n",
    "        acc = torch.div(correct*100, total, rounding_mode='trunc')\n",
    "        writer.add_scalar(\"Validation Accuracy(%) vs Epochs\", acc, epoch)\n",
    "        \n",
    "        print(\"validation loss for epoch {:2d}: {:5f}\".format(epoch+1, validation_loss))\n",
    "        print(\"validation acc for epoch {:2d}: {:5f}\".format(epoch+1, acc))\n",
    "        \n",
    "print('Finished Training')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c58eba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "008084f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fc5cc6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 95.44999694824219 %\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "#9th epoch\n",
    "model.load_state_dict(torch.load('./checkpoints/cifar_net{:02d}.pth'.format(12)))\n",
    "\n",
    "# Test the model on test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _,predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        #\n",
    "        #\n",
    "acc = torch.div(correct*100, total) #100 * correct // total #trunc floor\n",
    "print(f\"Accuracy of the network on the test images: {acc} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4402a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43024ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}